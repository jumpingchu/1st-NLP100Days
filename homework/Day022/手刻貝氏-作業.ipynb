{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手刻基本Naive Bayes模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 學習重點：理解單純貝氏模型原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize(message):\n",
    "    message = message.lower()\n",
    "    all_words = re.findall(\"[a-z0-9]+\", message)\n",
    "    return set(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀入資料並分割為 train/testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "paths =[r'../../data/spam_data/spam', r'../../data/spam_data/easy_ham', r'../../spam_data/hard_ham'] \n",
    "for path in paths:\n",
    "    for fn in glob.glob(path+\"/*\"):\n",
    "        if \"ham\" not in fn:\n",
    "            is_spam = True\n",
    "        else:\n",
    "            is_spam = False\n",
    "        #codecs.open可以避開錯誤，用errors='ignore'\n",
    "        with codecs.open(fn, encoding='utf-8', errors='ignore') as file:\n",
    "            for line in file:\n",
    "                #這個line的開頭為Subject:\n",
    "                if line.startswith(\"Subject:\"):\n",
    "                    subject = re.sub(r\"^Subject:\", \"\", line).strip()\n",
    "                    X.append(subject)\n",
    "                    Y.append(is_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# random_state 是為了讓各為學員得到相同的結果，平時可以移除\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for x_, y_ in zip(X_train, y_train):\n",
    "    train_data.append([x_, y_])\n",
    "\n",
    "for x_, y_ in zip(X_test, y_test):\n",
    "    test_data.append([x_, y_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Re: Goodbye Global Warming', False],\n",
       " ['Let us find the right mortgage lender for you      AFPE', True],\n",
       " ['[dgc.chat] First public release of NeuDist Distributed Transaction', False],\n",
       " ['Re: [VoID] a new low on the personals tip...', False],\n",
       " ['RE: Java is for kiddies', False]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Re: From', False],\n",
       " ['DVD capture: Unbreaking the Mac', False],\n",
       " ['Re: Goodbye Global Warming', False],\n",
       " ['=?ISO-2022-JP?B?GyRCTCQ+NUJ6OS05cCIoPF5HLiEqPVAycSQkJE45LT5sGyhC?=', True],\n",
       " ['Re: My source: RE: A biblical digression', False]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defaultdict用法示範"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dic : defaultdict(<function <lambda> at 0x7ff67fb19a60>, {'you': [1, 0], 'hi': [1, 2], 'no': [8, 1]})\n",
      "you : [1, 0]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "mess = 'This is our first time in Taiwan,,,,, such a beautiful country'\n",
    "\n",
    "counts = defaultdict(lambda:[0,0])\n",
    "counts['you'][0] += 1\n",
    "counts['hi'][0] += 1\n",
    "counts['hi'][1] += 2\n",
    "counts['no'][1] += 1\n",
    "counts['no'][0] += 8\n",
    "print('dic : {}'.format(counts))\n",
    "print('you : {}'.format(counts['you']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 創造一個字典，裡面是{'hi': [1, 0]}，對應第一個數字是是垃圾郵件的次數，對應第二個數字是不是垃圾郵件的次數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(training_set):\n",
    "    counts = defaultdict(lambda:[0,0])\n",
    "    for message, is_spam in training_set:\n",
    "        for word in tokenize(message):\n",
    "            '''自行填入， list[0]為出現在spam中的次數，list[1]為出現在ham(非spam)中的次數'''\n",
    "            counts[word][0 if is_spam else 1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算 p(w|spam) / p(w|non_spam)\n",
    "* 其中 K 為超參數，為了確保分母/分子皆不為 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "    #獲得三組數據，分別為w這個字，p(w|spam)，p(w|non_spam)\n",
    "    #counts[w][0]=spam \n",
    "    #counts[w][1]=non_spam\n",
    "    return [(w, (counts[w][0]+k)/(total_spams+2*k), (counts[w][1]+k)/(total_non_spams+2*k)) for w in counts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算貝氏結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_probability(word_probs, message, spam_prob, ham_prob):\n",
    "    \n",
    "    #先把這個mail的文字處理一下\n",
    "    message_words = tokenize(message)\n",
    "    \n",
    "    #初始化值=0\n",
    "    log_prob_spam = log_prob_ham = 0.0\n",
    "    \n",
    "    #將 w 這個字, p(w|spam), p(w|non_spam)依序引入\n",
    "    for word, word_on_spam, word_on_ham in word_probs:\n",
    "        \n",
    "        #假如這個字有在這個 mail 中出現\n",
    "        if word in message_words:\n",
    "            \n",
    "            #把他的 p(w|spam) 轉 log 值加上 log_prob_if_spam\n",
    "            log_prob_spam = log_prob_spam + math.log(word_on_spam)\n",
    "            \n",
    "            #把他的 p(w|non_spam) 轉 log 值加上 log_prob_if_not_spam\n",
    "            log_prob_ham = log_prob_ham + math.log(word_on_ham)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #如果沒出現 log_prob_if_spam ➕ 上的值＝1-p(w|spam)\n",
    "            #也就是這封信是垃圾郵件但是 w 這個字卻沒在裡面\n",
    "            log_prob_spam = log_prob_spam + math.log(1 - word_on_spam)\n",
    "            log_prob_ham = log_prob_ham + math.log(1 - word_on_ham)\n",
    "            \n",
    "    log_prob_spam = log_prob_spam + math.log(spam_prob)\n",
    "    log_prob_ham = log_prob_ham + math.log(ham_prob)\n",
    "    \n",
    "    #把 + 起來的值轉成 exp 再算 NaiveBayes\n",
    "    prob_spam = math.exp(log_prob_spam)\n",
    "    prob_ham = math.exp(log_prob_ham)\n",
    "    \n",
    "    #貝氏\n",
    "    return prob_spam / (prob_spam + prob_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打包整個模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    \n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "        \n",
    "    def train(self, training_set):\n",
    "        #訓練的資料格式為(message, is_spam)\n",
    "        \n",
    "        #所有垃圾郵件的數量\n",
    "        num_spams = len([is_spam for message, is_spam in training_set if is_spam])\n",
    "        \n",
    "        #所有不是垃圾郵件的數量\n",
    "        num_hams = len(training_set) - num_spams\n",
    "        \n",
    "        self.spam_probability = num_spams / len(training_set)\n",
    "        self.ham_probability = num_hams / len(training_set)\n",
    "        \n",
    "        #把 training_set 裡面的所有字體轉成 ('Bad', num_is_spam, num_not_spam)\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts, num_spams, num_hams, self.k)\n",
    "        \n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message, self.spam_probability, self.ham_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit 訓練集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.16250495441934204, 0.8374950455806579)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.spam_probability, classifier.ham_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('re', 0.06204379562043796, 0.49692526017029326),\n",
       " ('global', 0.0012165450121654502, 0.011589403973509934),\n",
       " ('goodbye', 0.0012165450121654502, 0.011116367076631977),\n",
       " ('warming', 0.0012165450121654502, 0.01064333017975402),\n",
       " ('for', 0.10097323600973236, 0.11565752128666036),\n",
       " ('us', 0.00851581508515815, 0.007805108798486282),\n",
       " ('the', 0.11070559610705596, 0.14120151371807002),\n",
       " ('mortgage', 0.025547445255474453, 0.00023651844843897824),\n",
       " ('afpe', 0.0036496350364963502, 0.00023651844843897824),\n",
       " ('find', 0.010948905109489052, 0.004493850520340586)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.word_probs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified = [(subject, is_spam, classifier.classify(subject)) for subject, is_spam in test_data]\n",
    "counts = Counter((is_spam, spam_probability > 0.5) for _, is_spam, spam_probability in classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(False, False): 527,\n",
       "         (True, True): 57,\n",
       "         (False, True): 11,\n",
       "         (True, False): 36})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 92.55%\n",
      "precision : 83.82%\n",
      "recall : 61.29%\n"
     ]
    }
   ],
   "source": [
    "precision = counts[(True, True)] / (counts[(True, True)] + counts[(False, True)])\n",
    "\n",
    "recall = counts[(True, True)] / (counts[(True, True)] + counts[(True, False)])\n",
    "\n",
    "all_result_counts = counts[(False, True)] + counts[(False, False)] + counts[(True, True)] + counts[(True, False)]\n",
    "binary_accuracy = (counts[(True, True)] + counts[(False, False)]) / all_result_counts\n",
    "\n",
    "print('accuracy : {:.2f}%'.format(binary_accuracy * 100))\n",
    "print('precision : {:.2f}%'.format(precision * 100))\n",
    "print('recall : {:.2f}%'.format(recall * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
