{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業目標：搭建一個 bag of words 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download()\n",
    "import numpy as np\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv',delimiter='\\t',quoting=3)\n",
    "corpus = dataset['Review'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 從文本中取出所有單字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jiaping/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#「punkt」包含了許多預訓練好的分詞模型\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_words = []\n",
    "for sentence in corpus:\n",
    "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "    for word in tokenized_sentence:\n",
    "        whole_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wow',\n",
       " '...',\n",
       " 'Loved',\n",
       " 'this',\n",
       " 'place',\n",
       " '.',\n",
       " 'Crust',\n",
       " 'is',\n",
       " 'not',\n",
       " 'good',\n",
       " '.',\n",
       " 'Not',\n",
       " 'tasty',\n",
       " 'and',\n",
       " 'the',\n",
       " 'texture',\n",
       " 'was',\n",
       " 'just',\n",
       " 'nasty',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 移除重複單字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12684"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(whole_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_words = set(whole_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有2351個單字\n"
     ]
    }
   ],
   "source": [
    "print('共有{}個單字'.format(len(unique_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立字典使每一個單字有對應數值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {}\n",
    "index_word = {}\n",
    "n = 0\n",
    "for word in unique_words:\n",
    "    index_word[n] = word\n",
    "    word_index[word] = n\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Wow'),\n",
       " (1, '...'),\n",
       " (2, 'Loved'),\n",
       " (3, 'this'),\n",
       " (4, 'place'),\n",
       " (5, '.'),\n",
       " (6, 'Crust'),\n",
       " (7, 'is'),\n",
       " (8, 'not'),\n",
       " (9, 'good'),\n",
       " (10, 'Not'),\n",
       " (11, 'tasty'),\n",
       " (12, 'and'),\n",
       " (13, 'the'),\n",
       " (14, 'texture'),\n",
       " (15, 'was'),\n",
       " (16, 'just'),\n",
       " (17, 'nasty'),\n",
       " (18, 'Stopped'),\n",
       " (19, 'by')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(index_word.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wow', 0),\n",
       " ('...', 1),\n",
       " ('Loved', 2),\n",
       " ('this', 3),\n",
       " ('place', 4),\n",
       " ('.', 5),\n",
       " ('Crust', 6),\n",
       " ('is', 7),\n",
       " ('not', 8),\n",
       " ('good', 9),\n",
       " ('Not', 10),\n",
       " ('tasty', 11),\n",
       " ('and', 12),\n",
       " ('the', 13),\n",
       " ('texture', 14),\n",
       " ('was', 15),\n",
       " ('just', 16),\n",
       " ('nasty', 17),\n",
       " ('Stopped', 18),\n",
       " ('by', 19)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_index.items())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 轉換句子為 bag of words 形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_bag_of_words_vector(sentence, word_index, whole_words):\n",
    "    sentence = sentence\n",
    "    v = np.zeros(len(whole_words)) # 創建指定長度的全 0 array (vector)\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        if word in whole_words:\n",
    "            v[word_index[word]] += 1 # 在 vector 對應 word 位置加 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2 = _get_bag_of_words_vector('Wow... Loved this place.', word_index, whole_words)\n",
    "v2[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = _get_bag_of_words_vector('Crust is not good. Wow', word_index, whole_words)\n",
    "v1[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
