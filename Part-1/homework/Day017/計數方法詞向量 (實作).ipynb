{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 瞭解如何使用計數方法詞向量與 SVD\n",
    "\n",
    "* 將文字資料輸入模型進行自然語言任務之前，其中一項重要的前處理即為將字詞向量化(詞嵌入 word embedding)。\n",
    "* 而將詞向量化的方法有很多，這裡我們會著重在介紹如何使用計數方法來將字詞向量化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 字詞前處理\n",
    "\n",
    "* 在進行字詞向量化之前，我們需要針對文本資料進行前置處理，將**文本資料分割成字詞(斷詞)**，再將分割後的**字詞轉換成字詞ID清單**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 導入會使用的 library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from typing import List  # 型別註釋(type annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義前處理函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed corpus: [[3 5 1 4 2 5 6 0]] \n",
      " word2idx: {'.': 0, 'goodbye': 1, 'i': 2, 'you': 3, 'and': 4, 'say': 5, 'hello': 6} \n",
      " idx2word: {0: '.', 1: 'goodbye', 2: 'i', 3: 'you', 4: 'and', 5: 'say', 6: 'hello'}\n"
     ]
    }
   ],
   "source": [
    "def preprocess(corpus: List[str], only_word: bool = False):\n",
    "    '''Function to do preprocess of input corpus\n",
    "    Parameters\n",
    "    -----------\n",
    "    corpus: str\n",
    "        input corpus to be processed\n",
    "    only_word: bool\n",
    "        whether to filter out non-word\n",
    "    '''\n",
    "    word_dic = set()\n",
    "    processed_sentence = []\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        # 將所有字詞轉為小寫\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        # 移除標點符號 (可以依據使用狀況決定是否要移除標點符號)\n",
    "        if only_word:\n",
    "            pattern = r'[^\\W_]+'\n",
    "            sentence = re.findall(pattern, sentence)\n",
    "        else:\n",
    "            punctuation_list = ['.', ',', '!', '?']\n",
    "            for pun in punctuation_list:\n",
    "                sentence = sentence.replace(pun, ' '+pun)\n",
    "            sentence = sentence.split(' ')\n",
    "        \n",
    "        # 添加字詞到字典中\n",
    "        word_dic |= set(sentence)  # in_place operation\n",
    "        processed_sentence.append(sentence)\n",
    "    \n",
    "    \n",
    "    # 建立字詞 ID 清單\n",
    "    word2idx = dict()\n",
    "    idx2word = dict()\n",
    "    for word in word_dic:\n",
    "        if word not in word2idx:\n",
    "            idx = len(word2idx)\n",
    "            word2idx[word] = idx\n",
    "            idx2word[idx] = word\n",
    "\n",
    "    # 將文本轉為 ID 型式\n",
    "    id_mapping = lambda x: word2idx[x]\n",
    "    \n",
    "    corpus = np.array([list(map(id_mapping, sentence)) for sentence in processed_sentence])\n",
    "\n",
    "    return corpus, word2idx, idx2word\n",
    "\n",
    "# 定義簡易文本資料 (使用講義中的例子)\n",
    "corpus = ['You say goodbye and I say hello.']\n",
    "\n",
    "processed_corpus, word2idx, idx2word = preprocess(corpus)\n",
    "print(f'Processed corpus: {processed_corpus} \\n word2idx: {word2idx} \\n idx2word: {idx2word}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 共現矩陣\n",
    "將轉化處理過的文本資料轉化為共現矩陣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義共現矩陣函式 (method 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 1, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 1],\n",
       "       [0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 1, 0, 0, 2, 0],\n",
       "       [1, 1, 1, 1, 2, 0, 1],\n",
       "       [1, 0, 1, 0, 0, 1, 0]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_co_matrix(corpus: np.ndarray, vocab_size: int, window_size: int=1):\n",
    "    # initialize co-occurrence matrix\n",
    "    co_matrix = np.zeros(shape=(vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        sentence_size = len(sentence)\n",
    "    \n",
    "        for idx, word_id in enumerate(sentence):\n",
    "            for i in range(1, window_size+1):\n",
    "                left_idx = idx - i\n",
    "                right_idx = idx + i\n",
    "\n",
    "                if left_idx >= 0:\n",
    "                    left_word_id = sentence[left_idx]\n",
    "                    co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "                if right_idx < sentence_size:\n",
    "                    right_word_id = sentence[right_idx]\n",
    "                    co_matrix[word_id, right_word_id] += 1\n",
    "                \n",
    "    return co_matrix\n",
    "\n",
    "co_matrix = create_co_matrix(processed_corpus, len(word2idx), 2)\n",
    "co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義共現矩陣函式 (method 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You say goodbye and I say hello.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 5, 1, 4, 2, 5, 6, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 1, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 1],\n",
       "       [0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 1, 0, 0, 2, 0],\n",
       "       [1, 1, 1, 1, 2, 0, 1],\n",
       "       [1, 0, 1, 0, 0, 1, 0]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_co_matrix(corpus: np.ndarray, vocab_size: int, window_size: int=1):\n",
    "    # initialize co-occurrence matrix\n",
    "    co_matrix = np.zeros(shape=(vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        sentence_size = len(sentence)\n",
    "    \n",
    "        for idx, word_id in enumerate(sentence):\n",
    "            left_idx = idx - window_size if idx - window_size >= 0 else 0\n",
    "            context_ids = sentence[left_idx:idx]\n",
    "            \n",
    "            for left_i, left_id in enumerate(context_ids):\n",
    "                co_matrix[word_id, left_id] += 1\n",
    "                co_matrix[left_id, word_id] += 1\n",
    "\n",
    "    return co_matrix\n",
    "\n",
    "co_matrix = create_co_matrix(processed_corpus, len(word2idx), 2)\n",
    "co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 向量相似度\n",
    "* 比較轉換為向量的字詞的方法有很多種，其中當要表示字詞的相似度時，最常使用的方法為餘弦相似度 (Cosine Similarity)\n",
    "\n",
    "$$\n",
    "sim(x,y) = \\frac{xy}{||x||||y||}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義餘弦相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067726510136"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cos_similarity(x: np.ndarray, y: np.ndarray, eps: float=1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x**2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y**2)) + eps)\n",
    "    \n",
    "    return np.dot(nx,ny)\n",
    "\n",
    "# calculate the similarity between I and you\n",
    "cos_similarity(co_matrix[word2idx['i']], co_matrix[word2idx['you']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立可供查詢相似度的函數\n",
    "* 輸入字詞，查詢與此字詞 top_n 相似的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_similarity(query: str, word2idx: dict, idx2word: dict, word_matrix: np.ndarray, top_k: int=3):\n",
    "    \n",
    "    # handle the situation of query word not in corpus\n",
    "    if query not in word2idx:\n",
    "        raise ValueError(f\"{query} is not found in input dictionary\")\n",
    "    \n",
    "    # handle the situation of top_k is the same as the amount of words\n",
    "    if top_k >= len(word2idx):\n",
    "        raise ValueError(f\"top_k needs to be less than the amount of words\")\n",
    "        \n",
    "    print(f\"[query] : {query}\")\n",
    "    query_id = word2idx[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "    \n",
    "    # calculate cosine similarity\n",
    "    similarity_scores = np.zeros(len(word2idx))\n",
    "    for i in range(len(word2idx)):\n",
    "        similarity_scores[i] = cos_similarity(query_vec, word_matrix[i])\n",
    "\n",
    "    # remove query word\n",
    "    similarity_scores[query_id] = 0\n",
    "    filter_word2idx = dict([(k, v) for k, v in word2idx.items() if k != query])\n",
    "    filter_idx2word = dict([(k, v) for k, v in idx2word.items() if k != query_id])\n",
    "    \n",
    "    # sorting by similarity score\n",
    "    top_k_idx = (-similarity_scores).argsort()[:top_k] # np.argsort(): 從小到大排列並提取對應的index\n",
    "    top_k_word = [filter_idx2word[word_idx] for word_idx in top_k_idx]\n",
    "    \n",
    "    return dict(zip(top_k_word, similarity_scores[top_k_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[query] : you\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'and': 0.8660253941251803, 'i': 0.7071067726510136, '.': 0.49999999292893216}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_similarity('you', word2idx, idx2word, co_matrix, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 正向點間互資訊 (PPMI)\n",
    "* 由於共生矩陣在高頻字上的缺陷，而PMI中加入了兩字詞共同出現的機率與各自出現的機率的關係，以此解決高頻詞在共生矩陣上的缺陷。\n",
    "\n",
    "* 而 PPMI 即將 PMI 內會產生負值的情況排除(若出現負值則賦予 0)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&PMI(x,y) = log_2\\frac{P(x,y)}{P(x)P(y)} = log_2\\frac{C(x,y)N}{C(x)C(y)} \\\\\n",
    "&PPMI(x,y) = max(0,PMI(x,y))\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義正向點間互資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(co_matrix: np.ndarray, eps: float=1e-8, verbose: bool=False):\n",
    "    # np.zeros_like(): 創建一個跟給定陣列相同大小的全 0 陣列\n",
    "    N = np.sum(co_matrix)\n",
    "    M = np.zeros_like(co_matrix, dtype=np.float32)\n",
    "    S = np.sum(co_matrix, axis=0)\n",
    "    total = co_matrix.shape[0] * co_matrix.shape[1]\n",
    "\n",
    "    cnt = 0\n",
    "    for i in range(co_matrix.shape[0]):\n",
    "        for j in range(co_matrix.shape[1]):\n",
    "            pmi = np.log2(co_matrix[i, j]*N / (S[i]*S[j] + eps))\n",
    "            M[i, j] = max(0, pmi)\n",
    "            \n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % 10 == 0 or cnt == total:\n",
    "                    print(f\"{cnt}/{total} Done\")\n",
    "    \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/49 Done\n",
      "20/49 Done\n",
      "30/49 Done\n",
      "40/49 Done\n",
      "49/49 Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in log2\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.       , 0.       , 0.       , 0.       , 0.       , 0.8930848,\n",
       "        2.1154773],\n",
       "       [0.       , 0.       , 0.7004397, 1.7004397, 0.7004397, 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.7004397, 0.       , 0.       , 0.7004397, 0.       ,\n",
       "        1.1154772],\n",
       "       [0.       , 1.7004397, 0.       , 0.       , 0.       , 0.8930848,\n",
       "        0.       ],\n",
       "       [0.       , 0.7004397, 0.7004397, 0.       , 0.       , 0.8930848,\n",
       "        0.       ],\n",
       "       [0.8930848, 0.       , 0.       , 0.8930848, 0.8930848, 0.       ,\n",
       "        0.3081223],\n",
       "       [2.1154773, 0.       , 1.1154772, 0.       , 0.       , 0.3081223,\n",
       "        0.       ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ppmi = ppmi(co_matrix, verbose=True)\n",
    "output_ppmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SVD\n",
    "* 觀察上面的 PPMI 輸出矩陣，可以發現大部分的元素都為 0 (稀疏矩陣)，因此可以發現此矩陣包含了許多無法提供訊息的元素\n",
    "* 利用奇異值分解，將矩陣降維，並保存重要的資訊。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 np 的 `linalg.svd()` 對 PPMI 矩陣進行奇異值分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello in co-occurrence matrix: [1 0 1 0 0 1 0]\n",
      "hello in PPMI: [2.1154773 0.        1.1154772 0.        0.        0.3081223 0.       ]\n",
      "hello in SVD: [-0.5126197   0.5698161  -0.39725903 -0.4323913  -0.01054526  0.124419\n",
      "  0.22839099]\n"
     ]
    }
   ],
   "source": [
    "# SVD\n",
    "U, S, V = np.linalg.svd(output_ppmi)\n",
    "\n",
    "# 使用SVD將將原本的稀疏向量轉變為稠密向量\n",
    "print(f\"hello in co-occurrence matrix: {co_matrix[word2idx['hello']]}\")\n",
    "print(f\"hello in PPMI: {output_ppmi[word2idx['hello']]}\")\n",
    "print(f\"hello in SVD: {U[word2idx['hello']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.3683197e-08 -7.4080468e-08  2.2216554e-08 -4.0760728e-08\n",
      "  -8.4676607e-08  8.9308482e-01  2.1154773e+00]\n",
      " [-6.8160695e-09  5.8041792e-08  7.0043969e-01  1.7004398e+00\n",
      "   7.0043969e-01  5.6385137e-09 -7.4432329e-08]\n",
      " [ 2.0622537e-08  7.0043969e-01  2.3153314e-08  3.5444103e-08\n",
      "   7.0043969e-01  2.2115112e-08  1.1154772e+00]\n",
      " [-4.8094705e-08  1.7004397e+00  3.7209105e-08 -4.0341142e-08\n",
      "   1.4502533e-09  8.9308476e-01 -4.3083389e-08]\n",
      " [-8.3527041e-08  7.0043969e-01  7.0043969e-01 -4.4883759e-08\n",
      "  -2.3019233e-08  8.9308482e-01 -4.3795090e-08]\n",
      " [ 8.9308482e-01  4.0985121e-08  5.5780403e-09  8.9308482e-01\n",
      "   8.9308482e-01  1.0392343e-08  3.0812228e-01]\n",
      " [ 2.1154773e+00 -1.3404321e-08  1.1154772e+00 -6.0040975e-08\n",
      "  -4.2378574e-08  3.0812228e-01 -5.6422177e-08]]\n",
      "[[0.        0.        0.        0.        0.        0.8930848 2.1154773]\n",
      " [0.        0.        0.7004397 1.7004397 0.7004397 0.        0.       ]\n",
      " [0.        0.7004397 0.        0.        0.7004397 0.        1.1154772]\n",
      " [0.        1.7004397 0.        0.        0.        0.8930848 0.       ]\n",
      " [0.        0.7004397 0.7004397 0.        0.        0.8930848 0.       ]\n",
      " [0.8930848 0.        0.        0.8930848 0.8930848 0.        0.3081223]\n",
      " [2.1154773 0.        1.1154772 0.        0.        0.3081223 0.       ]]\n"
     ]
    }
   ],
   "source": [
    "# 檢查分解是否正確\n",
    "# a @ b = dot(a, b)\n",
    "A = U @ np.diag(S) @ V  \n",
    "print(A)\n",
    "print(output_ppmi)\n",
    "# 可以發現 SVD 後，結果跟原來的 output_ppmi 是相同的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.9371588  2.5547988  2.1101685  1.9556583  1.1257027  0.58972406\n",
      " 0.30812874]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.48203   , -0.56445074,  0.26282662, -0.430857  , -0.33953863,\n",
       "        -0.2642201 ],\n",
       "       [-0.31352073,  0.30776063,  0.48896635,  0.5457005 , -0.38465765,\n",
       "         0.12412582],\n",
       "       [-0.33312967, -0.30777904,  0.16466641,  0.03673923,  0.5294517 ,\n",
       "         0.6964652 ],\n",
       "       [-0.29432744, -0.29746115, -0.5294562 ,  0.511355  ,  0.22169203,\n",
       "        -0.35262936],\n",
       "       [-0.26702777, -0.09261478, -0.3523957 ,  0.24547683, -0.44945022,\n",
       "         0.26410997],\n",
       "       [-0.3710324 ,  0.26495245,  0.31999645,  0.0807369 ,  0.45295563,\n",
       "        -0.4691856 ],\n",
       "       [-0.5126197 ,  0.5698161 , -0.39725903, -0.4323913 , -0.01054526,\n",
       "         0.124419  ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以發現前六個奇異值就佔了絕大多數的奇異值\n",
    "print(S)\n",
    "\n",
    "# 可以取前六個維度當作降維的詞向量\n",
    "U_reduce = U[:, 0:6]\n",
    "U_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 繪製視覺化圖表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAbkElEQVR4nO3de3RU9b338feXEEyOyESRYgoqsaIHCHIbKGoBqyLpKl4oR1vqBaWaqvVZteepq3TRh3rrWgqeeivL58QqXupZUOCo1EsEsR6Kl0KigECq4dYDaYw5auYxMNFgvs8fGdKQJiRkTzIT9ue1Vlb23vOb/fv+smE+s/fs2dvcHRERCZ9eqS5ARERSQwEgIhJSCgARkZBSAIiIhJQCQEQkpHqnuoC2nHjiiT5kyJBUlyEi0qOUlpb+j7sP6EjbtA2AIUOGUFJSkuoyRER6FDP7a0fbJuUQkJkVmNn7ZrbdzOa20eYKM9tmZlvN7D+S0a+IiHRe4AAwswxgEfAtYDgwy8yGt2gzFPg5cK67jwBuDdpvR+zevZv8/PwOt7/99tu57777ALj22mtZvnx5V5UmIpJyydgDmABsd/ed7v4FsAS4tEWbG4BF7v4pgLt/lIR+RUQkgGQEwCBgT7P5vYllzZ0BnGFmb5jZ22ZW0NqKzKzQzErMrKS6ujoJpcGXX37JDTfcwIgRI7jooouIx+Ps2LGDgoICxo0bx6RJk/jLX/5y2HWsWbOGMWPGMHLkSObMmcPnn3+elNpERFKpu04D7Q0MBc4DZgGPmllOy0buXuTuUXePDhjQoQ+x21VeXs6PfvQjtm7dSk5ODitWrKCwsJCHH36Y0tJS7rvvPm6++eY2n19XV8e1117L0qVLee+99zhw4ACPPPJIUmoTEUmlZJwFVAGc3Gx+cGJZc3uBP7t7PbDLzD6gMRA2JKH/Q5RVxijeUkVFTZzsuo8ZdMqpjB49GoBx48axe/du3nzzTS6//PKm5xzuHf37779PXl4eZ5xxBgCzZ89m0aJF3HrrrckuXUSkWyUjADYAQ80sj8YX/u8B32/R5jka3/kvNrMTaTwktDMJfR+irDJG0dpdRLIzyY1ksafmAPvqjbLKGMNyI2RkZFBVVUVOTg4bN25MdvciIj1K4ENA7n4AuAV4BSgDfu/uW83sTjO7JNHsFeBjM9sG/BG4zd0/Dtp3S8VbqohkZxLJzqSXGcdl9aZXL6N4S1VTm379+pGXl8eyZcsO1s+mTZvaXOeZZ57J7t272b59OwBPP/00U6ZMSXbpIiLdLimfAbj7S+5+hrt/zd1/lVg2391XJqbd3f/V3Ye7+0h3X5KMfluqqIlzXNahOzW9zKioiR+y7JlnnuGxxx5j1KhRjBgxgueff77NdWZlZbF48WIuv/xyRo4cSa9evbjxxhu7onwRkW5l6XpDmGg06kf6TeD7V39ALF5PJDuzadnB+Z9MPSPZJYqIpB0zK3X3aEfaHlUXgyvIH0gsXk8sXk+De9N0Qf7AVJcmIpJ2jqoAGJYboXByHpHsTCpjdUSyMymcnMew3EiqSxMRSTtpezG4zhqWG9ELvohIBxxVewAiItJxCgARkZBSAIiIhJQCQEQkpBQAIiIhpQAQEQkpBYCISEgpAEREQkoBICISUgoAEZGQUgCIiISUAkBEJKQUACIiIaUAEBEJKQWAiEhIKQBEREJKASAiElIKABGRkFIAiIiElAJARCSkFAAiIiGlABARCamkBICZFZjZ+2a23czmHqbdTDNzM4smo18REem8wAFgZhnAIuBbwHBglpkNb6XdccCPgT8H7VNERIJLxh7ABGC7u+909y+AJcClrbS7C7gXqEtCnyIiElAyAmAQsKfZ/N7EsiZmNhY42d1fPNyKzKzQzErMrKS6ujoJpYmISFu6/ENgM+sF/Br43+21dfcid4+6e3TAgAFdXZqISKglIwAqgJObzQ9OLDvoOCAfeN3MdgMTgZX6IFhEJLWSEQAbgKFmlmdmfYDvASsPPujuMXc/0d2HuPsQ4G3gEncvSULfIiLSSYEDwN0PALcArwBlwO/dfauZ3WlmlwRdv4iIdI3eyViJu78EvNRi2fw22p6XjD5FRCQYfRNYRCSkFAAiIiGlABARCSkFgIiE3u7du8nPzwfgiSee4JZbbklxRd1DASAiElIKABHpce666y7OPPNMvvGNbzBr1izuu+8+Nm7cyMSJEznrrLOYMWMGn376KUCby0tLSxk1ahSjRo1i0aJFh6x/z549nHfeeQwdOpQ77rgDgPnz5/PAAw80tZk3bx4PPvggAAsXLmT8+PGcddZZ/PKXv+yGv0ByKABEpEfZsGEDK1asYNOmTbz88suUlDR+p/Saa67h3nvvZfPmzYwcObLphbut5ddddx0PP/wwmzZt+oc+1q9fz4oVK9i8eTPLli2jpKSEOXPm8NRTTwHQ0NDAkiVLuOqqq1i1ahXl5eWsX7+ejRs3Ulpaytq1a7vprxFMUr4HICLSlcoqYxRvqaKiJs72157lnPOnkZWVRVZWFhdffDH79u2jpqaGKVOmADB79mwuv/xyYrFYq8tramqoqalh8uTJAFx99dW8/PLLTf1NnTqV/v37A/Cd73yHdevWceutt9K/f3/effddqqqqGDNmDP3792fVqlWsWrWKMWPGAFBbW0t5eXnTutOZAkBE0lpZZYyitbuIZGeSG8liS30D71TXUFYZY1hupEv6NLNW56+//nqeeOIJPvzwQ+bMmQOAu/Pzn/+cH/7wh11SS1fSISARSWvFW6qIZGcSyc6klxnDRkf567t/4g/v/De1tbW88MILHHvssRx//PH86U9/AuDpp59mypQpRCKRVpfn5OSQk5PDunXrAHjmmWcO6XP16tV88sknxONxnnvuOc4991wAZsyYQXFxMRs2bGDatGkATJs2jccff5za2loAKioq+Oijj7rlbxOU9gBEJK1V1MTJjWQ1zZ9y5lmMPOd87rlhOn/42imMHDmSSCTCk08+yY033sj+/fs57bTTWLx4MUCbyxcvXsycOXMwMy666KJD+pwwYQIzZ85k7969XHXVVUSjjRcv7tOnD9/85jfJyckhIyMDgIsuuoiysjLOPvtsAPr27cvvfvc7vvKVr3T53yYoc/dU19CqaDTqBz/cEZHwun/1B8Ti9USyM5uWVX8SY8AJEX547mAmT55MUVERY8eO7fJaGhoaGDt2LMuWLWPo0KFd3l9nmFmpu3focvs6BCSShvbt28e3v/1tRo0aRX5+PkuXLuXOO+9k/Pjx5OfnU1hYiLuzY8eOQ174ysvLu+WFsDsV5A8kFq8nFq+nwZ1YvJ7/fGg+/37rTMaOHcvMmTO7Zczbtm3j9NNP54ILLkjbF/8jpUNAImmouLiYr371q7z4YuNdVGOxGFOnTmX+/MaL7F599dW88MILXHzxxUQiETZu3Mjo0aNZvHgx1113XSpLT7phuREKJ+c1nQU0KCebZ5cv6bIPgNsyfPhwdu7c2a19djUFgEiaaH6qY2ZtX14qfoWf/exnTJ8+nUmTJrFixQoWLFjA/v37+eSTTxgxYgQXX3wx119/PYsXL+bXv/41S5cuZf369akeStINy410+wt+GCgARNJAy1MdPztmMJfc/jQnxN/nF7/4BRdccAGLFi2ipKSEk08+mdtvv526ujoAZs6cyR133MH555/PuHHjms5fF2mPPgMQSQMtT3Vk/yf0jxxHnzPP47bbbuOdd94B4MQTT6S2tpbly5c3PTcrK4tp06Zx0003HXWHf6RraQ9AJA20PNWxctcH/OHRBRxogFMH9OORRx7hueeeIz8/n5NOOonx48cf8vwrr7ySZ5999h9OZxQ5HAWASBoYlJN9yKmO/xydRO6IiUSyM/nJ1DMAiEaj3H333a0+f926dVx33XVN56aLdIQCQCQNFOQPpGjtLgCOy+rNZ3UHiMXr+e74we0+d8aMGezYsYPXXnutq8uUo4y+CCaSJpqfBTQoJ5uC/IE680WO2JF8EUx7ACJpQqc6SnfTWUAiIiGlABARCSkFgIhISCkARERCKikBYGYFZva+mW03s7mtPP6vZrbNzDab2RozOzUZ/YqISOcFDgAzywAWAd8ChgOzzGx4i2bvAlF3PwtYDiwI2q+IiASTjD2ACcB2d9/p7l8AS4BLmzdw9z+6+/7E7NtA+99uERGRLpWMABgE7Gk2vzexrC0/AF5u7QEzKzSzEjMrqa6uTkJpIiLSlm79ENjMrgKiwMLWHnf3InePunt0wIAB3VmaiEjoJOObwBXAyc3mByeWHcLMLgTmAVPc/fMk9CsiIgEkYw9gAzDUzPLMrA/wPWBl8wZmNgb4d+ASd/8oCX2KiEhAgQPA3Q8AtwCvAGXA7919q5ndaWaXJJotBPoCy8xso5mtbGN1IiLSTZJyMTh3fwl4qcWy+c2mL0xGPyIikjz6JrCISEgpAEREQkoBICISUgoAEZGQUgCIiISUAkBEJKQUACIiIaUAEBEJKQWAiEhIKQBEREJKASAiElIKABGRkFIAiIiElAJARCSkFAAiIiGlABARCSkFgIhISCkARERCSgEgIhJSCgARkZBSAIiIhJQCQEQkpBQAIiIhpQAQEQkpBYCISEglJQDMrMDM3jez7WY2t5XHjzGzpYnH/2xmQ5LRr4iIdF7gADCzDGAR8C1gODDLzIa3aPYD4FN3Px24H7g3aL8iIhJMMvYAJgDb3X2nu38BLAEubdHmUuDJxPRy4AIzsyT0LSIinZSMABgE7Gk2vzexrNU27n4AiAH9k9C3iIh0Ulp9CGxmhWZWYmYl1dXVqS5HROSolowAqABObjY/OLGs1TZm1huIAB+3XJG7F7l71N2jAwYMSEJpIiLSlmQEwAZgqJnlmVkf4HvAyhZtVgKzE9P/Arzm7p6EvkVEpJN6B12Bux8ws1uAV4AM4HF332pmdwIl7r4SeAx42sy2A5/QGBIiIpJCgQMAwN1fAl5qsWx+s+k64PJk9CUiIsmRVh8Ci4hI91EAiIiElAJARCSkFAAiIiGlABAROQqZWW17bRQAIiIhpQAQEUlTl112GePGjWPEiBEUFRUB0LdvX+bNm8eoUaOYOHEiVVVVAOzatYuzzz4bYLiZ3d2R9SsARETS1OOPP05paSklJSU89NBDfPzxx+zbt4+JEyeyadMmJk+ezKOPPgrAj3/8Y2666SaAbUBlR9aflC+CiYhIcGWVMYq3VFFRE2dQTjbbix9n3asvA7Bnzx7Ky8vp06cP06dPB2DcuHGsXr0agDfeeIMVK1Ywe/ZsgKfpwH1XFAAiImmgrDJG0dpdRLIzyY1ksWn9G6x+8RX+8GIxY7+Wy3nnnUddXR2ZmZkcvJ1KRkYGBw4caFrHkd5mRQEgIpIGirdUEcnOJJKdCUDGgTh9+0X4r52f8U/1Md5+++3DPv/cc89lyZIlB2ev7EifCgARkTRQURMnN5LVNP/P0cm88cIS7r6ugP+KnsXEiRMP+/wHH3yQ73//+9B4a96WN+VqlaXrVZmj0aiXlJSkugwRkW5x/+oPiMXrm/YAgKb5n0w9o8PrMbNSd492pK3OAhIRSQMF+QOJxeuJxetpcG+aLsgf2GV9KgBERNLAsNwIhZPziGRnUhmrI5KdSeHkPIblRrqsT30GICKSJoblRrr0Bb8l7QGIiISUAkBEJKQUACIiIaUAEBEJKQWAiEhIKQBEREJKASAiElIKABGRkFIAiIiElAJARCSkAgWAmZ1gZqvNrDzx+/hW2ow2s7fMbKuZbTaz7wbpU0REkiPoHsBcYI27DwXWJOZb2g9c4+4jgALgATPLCdiviIgEFDQALgWeTEw/CVzWsoG7f+Du5YnpvwEfAQMC9isiIgEFDYCB7n7w7vMfAoe9cLWZTQD6ADvaeLzQzErMrKS6ujpgaSIicjjtXg7azF4FTmrloXnNZ9zdzazN24uZWS6Nd6qf7e4NrbVx9yKgCBrvCNZebSIi0nntBoC7X9jWY2ZWZWa57l6ZeIH/qI12/YAXgXnufvg7G4uISLcIeghoJTA7MT0beL5lAzPrAzwLPOXuywP2JyIiSRI0AO4BpppZOXBhYh4zi5rZbxNtrgAmA9ea2cbEz+iA/YqISEDmnp6H2qPRqJeUlKS6DBGRHsXMSt092pG2+iawiEhIKQBEREJKASAiElIKABGRkFIAiIiElAJARCSkFAAiklTnnHNOqkvotPnz5/PAAw80zc+bN48HH3yQ2267jfz8fEaOHMnSpUsBeP3115k+fXpT21tuuYUnnniimysORgEgIkn15ptvprqETpszZw5PPfUUAA0NDSxZsoTBgwezceNGNm3axKuvvsptt91GZWVlO2vqGdq9FpCIyJHo27cvtbW1qS7jiJRVxijeUkVFTZx9ZLNi1VqObdjPmDFjWLduHbNmzSIjI4OBAwcyZcoUNmzYQL9+/VJddmAKABEJtbLKGEVrdxHJziQ3ksXIC2Zw9/3/l5My6/hfN17P6tWrW31e7969aWj4+4WN6+rquqvkpNEhIBEJrKwyxv2rP+CnyzZR/6VTVhlLdUkdVrylikh2JpHsTHqZ8fVvFrBn81us37CBadOmMWnSJJYuXcqXX35JdXU1a9euZcKECZx66qls27aNzz//nJqaGtasWZPqoRwx7QGISCAt30E7TtHaXRROzmNYbiTV5bWroiZObiSrab53Zh+Gjv46X2b+ExkZGcyYMYO33nqLUaNGYWYsWLCAk05qvEXKFVdcQX5+Pnl5eYwZMyZVQ+g0BYCIBNL8HTSAYUSyMyneUtUjAmBQTjaxeH1T/Q0NDewq28ic+Q8BYGYsXLiQhQsX/sNzFyxYwIIFC7q13mTSISARCaSiJs5xWYe+lzwuqzcVNfEUVXRkCvIHEovXE4vX87fd5dw9eyqDho/n6mlfT3VpXU57ACISSMt30PesfJdYvJ5BOdkprqxjhuVGKJycR/GWKmpzBjP/6TUU5A/sEXsvQSkARCSQgvyBFK3dBTS+8/+s7gCxeD3fHT84xZV13LDcSChe8FvSISARCeTgO+hIdiaVsToi2Zk95gPgsNMegIgEFtZ30D2d9gBEREJKASAiElIKABGRkFIAiIiElAJARCSkFAAiIiGlABARCSkFgIhISAUKADM7wcxWm1l54vfxh2nbz8z2mtlvgvQpIiLJEXQPYC6wxt2HAmsS8225C1gbsD8REUmSoAFwKfBkYvpJ4LLWGpnZOGAgsCpgfyIikiRBA2Cgu1cmpj+k8UX+EGbWC/g34KftrczMCs2sxMxKqqurA5YmIiKH0+7F4MzsVeCkVh6a13zG3d3MvJV2NwMvufteMztsX+5eBBQBRKPR1tYlIiJJ0m4AuPuFbT1mZlVmluvulWaWC3zUSrOzgUlmdjPQF+hjZrXufrjPC0REpIsFvRz0SmA2cE/i9/MtG7j7lQenzexaIKoXfxGR1Av6GcA9wFQzKwcuTMxjZlEz+23Q4kREpOuYe3oeao9Go15SUpLqMkREehQzK3X3aEfa6pvAIiIhpQAQEQkpBYCISEgpAEREQkoBICISUgoAEZGQUgCIiISUAkBEJKQUACIiIaUAEBEJKQWAiEhIKQBEREJKASAiElIKABGRkFIAiIiElAJARCSkFAAiIiGlABARCSkFgIhISCkARERCqneqCwiLssoYxVuqqKiJMygnm4L8gQzLjaS6LBEJMe0BdIOyyhhFa3cRi9eTG8kiFq+naO0uyipjqS5NREJMAdANirdUEcnOJJKdSS+zpuniLVWpLk1EQkwB0A0qauIcl/X3o21F826gYd/HVNTEU1iViISdAqAbDMrJ5rO6A03zhb96lF7H9mdQTnYKqxKRsFMAdIOC/IHE4vXE4vU0uDdNF+QPTHVpIhJigQLAzE4ws9VmVp74fXwb7U4xs1VmVmZm28xsSJB+e5phuREKJ+cRyc6kMlZHJDuTwsl5OgtIRFIq6Gmgc4E17n6Pmc1NzP+slXZPAb9y99Vm1hdoCNhvjzMsN6IXfBFJK0EPAV0KPJmYfhK4rGUDMxsO9Hb31QDuXuvu+wP2KyIiAQUNgIHuXpmY/hBo7aD2GUCNmf2nmb1rZgvNLKO1lZlZoZmVmFlJdXV1wNJERORw2j0EZGavAie18tC85jPu7mbmbfQxCRgD/DewFLgWeKxlQ3cvAooAotFoa+sSEZEkaTcA3P3Cth4zsyozy3X3SjPLBT5qpdleYKO770w85zlgIq0EgIiIdJ+gh4BWArMT07OB51tpswHIMbMBifnzgW0B+xURkYDMvfNHWsysP/B74BTgr8AV7v6JmUWBG939+kS7qcC/AQaUAoXu/kU7665OrDMVTgT+J0V9p5LGHS4a99HpVHcf0H6zgAFwtDKzEnePprqO7qZxh4vGLfomsIhISCkARERCSgHQuqJUF5AiGne4aNwhp88ARERCSnsAIiIhpQAQEQmp0AbAEVzK+ksz25j4WdlseZ6Z/dnMtpvZUjPr033Vd15Hx51o28/M9prZb5ote93M3m/2N/lK91QeTBLGPc7M3kts74fMzLqn8mA6Mm4zO9XM3klsz61mdmOzx47a7d3OuHvk9j5SoQ0A/n4p66HAmsR8a+LuPjrxc0mz5fcC97v76cCnwA+6ttyk6ei4Ae4C1ray/Mpmf5PWLv+RjoKO+xHgBmBo4qegK4rsAh0ZdyVwtruPBr4OzDWzrzZ7/Gjd3ocbd0/d3kckzAHQ7qWs25J4N3A+sLwzz0+xDo3bzMbReHXXVd1TVpfr9LgT17nq5+5ve+NZE0+19fw01O643f0Ld/88MXsMR8frQqfH3cO39xE5GjZ0Z3XkUtYAWYlLVL9tZpcllvUHatz94I1+9wKDuq7UpGp33GbWi8ZLd/y0jXUsTuw2/58etGscZNyDaNzGBx1V2xvAzE42s83AHuBed/9bs4ePyu0NbY67J2/vIxL0jmBpLQmXsobG62pUmNlpwGtm9h4QS3KpSZWEcd8MvOTue1v5/35l4u9xHLACuJrGd0gp18XjTlvJ+Hfu7nuAsxKHQJ4zs+XuXsXRvb1bHXfyK01fR3UAJOFS1rh7ReL3TjN7ncb7Gqyg8QqnvRN7AYOBiqQPoJOSMO6zgUlmdjPQF+hjZrXuPrfZ3+MzM/sPYAJp8oLQVeMGHqRxGx90tG3v5uv6m5ltofEeHsuP8u3dfF3Nx/0Gaby9kynMh4DavZS1mR1vZsckpk8EzgW2JY4L/hH4l8M9P021O253v9LdT3H3ITQeDnnK3eeaWe/E3wEzywSmA1u6p+zAOj3uxKGE/2dmExOHQK5p7flpqiP/zgebWXZi+njgG8D7R/v2bmvcPXx7Hxl3D+UPjcfx1wDlwKvACYnlUeC3ielzgPeATYnfP2j2/NOA9cB2YBlwTKrHlKxxt2h/LfCbxPSxNF7OezOwlcZ3xhmpHlNXj7tZuy3ADuA3JL5Fn+4/Hfx3PjWxTTclfheGYXu3Ne6evL2P9EeXghARCakwHwISEQk1BYCISEgpAEREQkoBICISUgoAEZGQUgCIiISUAkBEJKT+P7eg6aw3hCYWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 將詞向量降為二維方便視覺化\n",
    "U_visualization = U[:, 0:2]\n",
    "\n",
    "# visualization\n",
    "for word, word_id in word2idx.items():\n",
    "    plt.annotate(word, (U_reduce[word_id, 0], U_reduce[word_id, 1]))\n",
    "    \n",
    "plt.scatter(U_reduce[:, 0], U_reduce[:, 1], alpha=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
